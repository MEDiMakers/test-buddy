{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "def prepare_cross_encoder_dataset(json_data):\n",
    "    dataset_samples = []\n",
    "    for item in json_data:\n",
    "        question = item['Question']\n",
    "\n",
    "        # Process correct and similar answers with a score of 1\n",
    "        relevant_contexts = [item['Answer']] + item['Similar_answers']\n",
    "        for context in relevant_contexts:\n",
    "            sample = {\n",
    "                'query': question,\n",
    "                'context': context,\n",
    "                'score': 1\n",
    "            }\n",
    "            dataset_samples.append(sample)\n",
    "\n",
    "        # Process poor answers with a score of 0\n",
    "        for context in item['Poor_answers']:\n",
    "            sample = {\n",
    "                'query': question,\n",
    "                'context': context,\n",
    "                'score': 0\n",
    "            }\n",
    "            dataset_samples.append(sample)\n",
    "\n",
    "    return dataset_samples\n",
    "\n",
    "# Load data\n",
    "json_data = load_json('../data/Final_QA_pairs.json')\n",
    "\n",
    "# Prepare dataset\n",
    "cross_encoder_dataset = prepare_cross_encoder_dataset(json_data)\n",
    "\n",
    "# Save the prepared dataset\n",
    "save_json(cross_encoder_dataset, '../data/Cross_Encoder_Finetuning_Dataset.json')\n",
    "\n",
    "print(\"Dataset prepared and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset sample size: 40\n",
      "Training dataset sample size: 1560\n",
      "Datasets saved successfully.\n",
      "Test dataset preview:\n",
      "                                               query  \\\n",
      "0  A nurse is assessing a patient who has been re...   \n",
      "1  A nurse is assessing a patient who has been re...   \n",
      "2  A nurse is responding to a code blue in the em...   \n",
      "3  A nurse is responding to a code blue in the em...   \n",
      "4  A patient has been electrocuted and is now unr...   \n",
      "\n",
      "                                             context  score  \n",
      "0  (C) Chest Compressions Site of chest compressi...      1  \n",
      "1  Placement of Defibrillation Pads for Children/...      0  \n",
      "2  1.6:  OTHER  COMMON  CAUSES  OF  CARDIAC ARRES...      1  \n",
      "3  3.2: AUTOMATED EXTERNAL DEFIBRILLATORS (AEDs):...      0  \n",
      "4  (A) Ask someone to get an Automated External D...      1  \n",
      "Training dataset preview:\n",
      "                                               query  \\\n",
      "0  A nurse is assessing a patient who has been re...   \n",
      "1  A nurse is assessing a patient who has been re...   \n",
      "2  A nurse is assessing a patient who has been re...   \n",
      "3  A nurse is assessing a patient who has been re...   \n",
      "4  A nurse is assessing a patient who has been re...   \n",
      "\n",
      "                                             context  score  \n",
      "0  Based on national health statistics from the M...      0  \n",
      "1  The heart is a muscular pump located in the ce...      0  \n",
      "2  The heart muscles receive oxygen rich blood vi...      0  \n",
      "3  On an electrocardiogram (ECG), normal heart rh...      0  \n",
      "4  1.3: RISK FACTORS FOR HEART ATTACK Survival ra...      0  \n",
      "1560\n"
     ]
    }
   ],
   "source": [
    "#load from csv\n",
    "import pandas as pd\n",
    "data = pd.read_csv('../data/finetuning/ce_finetuning_dataset(41-60).csv', header=None, names=['query', 'context', 'score'])\n",
    "# Prepare the test and train datasets\n",
    "test_dataset = pd.DataFrame()\n",
    "train_dataset = pd.DataFrame()\n",
    "\n",
    "# Iterate over each group, separating test and train samples\n",
    "for _, group in data.groupby('query'):\n",
    "    positives = group[group['score'] == 1]\n",
    "    negatives = group[group['score'] == 0]\n",
    "    \n",
    "    # Ensure there is at least one positive and one negative for the test dataset\n",
    "    if not positives.empty and not negatives.empty:\n",
    "        selected_positive = positives.sample(n=1)\n",
    "        selected_negative = negatives.sample(n=1)\n",
    "        \n",
    "        # Append selected samples to the test dataset\n",
    "        test_dataset = pd.concat([test_dataset, selected_positive, selected_negative])\n",
    "\n",
    "        # Append the remaining data to the training dataset\n",
    "        # We drop the selected samples by index from the original group\n",
    "        remaining_samples = group.drop(selected_positive.index).drop(selected_negative.index)\n",
    "        train_dataset = pd.concat([train_dataset, remaining_samples])\n",
    "\n",
    "# Reset indices for clean datasets\n",
    "test_dataset.reset_index(drop=True, inplace=True)\n",
    "train_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Test dataset sample size:\", len(test_dataset))\n",
    "print(\"Training dataset sample size:\", len(train_dataset))\n",
    "# Save test and train datasets to CSV files\n",
    "test_dataset.to_csv('../data/finetuning/test_dataset.csv', index=False)\n",
    "train_dataset.to_csv('../data/finetuning/train_dataset.csv', index=False)\n",
    "\n",
    "print(\"Datasets saved successfully.\")\n",
    "print(\"Test dataset preview:\")\n",
    "print(test_dataset.head())\n",
    "print(\"Training dataset preview:\")\n",
    "print(train_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-legacy in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (0.9.48)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (3.10.2)\n",
      "Requirement already satisfied: dataclasses-json in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (2024.6.1)\n",
      "Requirement already satisfied: httpx in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (3.9)\n",
      "Requirement already satisfied: numpy in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (1.40.3)\n",
      "Requirement already satisfied: pandas in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (0.9.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-legacy) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-legacy) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-legacy) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-legacy) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-legacy) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-legacy) (1.9.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-legacy) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-legacy) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-legacy) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-legacy) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-legacy) (4.66.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-legacy) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-legacy) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-legacy) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-legacy) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-legacy) (1.3.1)\n",
      "Requirement already satisfied: certifi in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from httpx->llama-index-legacy) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from httpx->llama-index-legacy) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from httpx->llama-index-legacy) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-legacy) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-legacy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-legacy) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-legacy) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-legacy) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-legacy) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from pandas->llama-index-legacy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from pandas->llama-index-legacy) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from pandas->llama-index-legacy) (2024.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-legacy) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-legacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-legacy) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-legacy\n",
    "%pip install llama-index-finetuning\n",
    "%pip install llama-index-llms-openai\n",
    "!pip install huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41e9071fac04d76ac1ba7becd08da07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492db2d856af4d7db7463fc025ea0152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94345567073406e93345e1f7d9c6fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_dataset = pd.read_csv('../data/finetuning/train_dataset.csv')\n",
    "class CrossEncoderFinetuningDatasetSample:\n",
    "    def __init__(self, query, context, score):\n",
    "        self.query = query\n",
    "        self.context = context\n",
    "        self.score = score\n",
    "finetuning_dataset = [CrossEncoderFinetuningDatasetSample(d[0], d[1], d[2]) for d in train_dataset.values]\n",
    "print(len(finetuning_dataset))\n",
    "\n",
    "from llama_index.legacy.finetuning.cross_encoders.cross_encoder import CrossEncoderFinetuneEngine\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialise the cross-encoder fine-tuning engine\n",
    "finetuning_engine = CrossEncoderFinetuneEngine(\n",
    "    dataset=finetuning_dataset, epochs=2, batch_size=8\n",
    ")\n",
    "\n",
    "# Finetune the cross-encoder model\n",
    "finetuning_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a8ebad6b874b6bbf532f36db6f4013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67c12e146ca486fb09cb9a08cefb1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "finetuning_engine.push_to_hub(\n",
    "    repo_id=\"ethan-cyj/Cross-Encoder-Finetuned\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463903248ac3462c9a9d302bc3aaf65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "base_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "finetuned_model = CrossEncoder('ethan-cyj/Cross-Encoder-Finetuned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Base Accuracy': 0.725, 'Base Precision': 1.0, 'Base Recall': 0.45, 'Base F1': 0.6206896551724138, 'Finetuned Accuracy': 0.825, 'Finetuned Precision': 0.8823529411764706, 'Finetuned Recall': 0.75, 'Finetuned F1': 0.8108108108108109}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "df = pd.read_csv('../data/finetuning/test_dataset.csv')\n",
    "for index, row in df.iterrows():\n",
    "    query = row['query']\n",
    "    context = row['context']\n",
    "    base_score = base_model.predict([(query, context)])[0]\n",
    "    finetuned_score = finetuned_model.predict([(query, context)])[0]\n",
    "    results.append({\n",
    "        'query': query,\n",
    "        'context': context,\n",
    "        'label': row['score'],\n",
    "        'base_score': base_score,\n",
    "        'finetuned_score': finetuned_score\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assume a threshold for classification\n",
    "threshold = 0.5\n",
    "results_df['base_pred'] = (results_df['base_score'] >= threshold).astype(int)\n",
    "results_df['finetuned_pred'] = (results_df['finetuned_score'] >= threshold).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'Base Accuracy': accuracy_score(results_df['label'], results_df['base_pred']),\n",
    "    'Base Precision': precision_score(results_df['label'], results_df['base_pred']),\n",
    "    'Base Recall': recall_score(results_df['label'], results_df['base_pred']),\n",
    "    'Base F1': f1_score(results_df['label'], results_df['base_pred']),\n",
    "    'Finetuned Accuracy': accuracy_score(results_df['label'], results_df['finetuned_pred']),\n",
    "    'Finetuned Precision': precision_score(results_df['label'], results_df['finetuned_pred']),\n",
    "    'Finetuned Recall': recall_score(results_df['label'], results_df['finetuned_pred']),\n",
    "    'Finetuned F1': f1_score(results_df['label'], results_df['finetuned_pred'])\n",
    "}\n",
    "\n",
    "# Display metrics\n",
    "print(metrics)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
