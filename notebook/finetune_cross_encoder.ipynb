{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "def prepare_cross_encoder_dataset(json_data):\n",
    "    dataset_samples = []\n",
    "    for item in json_data:\n",
    "        question = item['Question']\n",
    "\n",
    "        # Process correct and similar answers with a score of 1\n",
    "        relevant_contexts = [item['Answer']] + item['Similar_answers']\n",
    "        for context in relevant_contexts:\n",
    "            sample = {\n",
    "                'query': question,\n",
    "                'context': context,\n",
    "                'score': 1\n",
    "            }\n",
    "            dataset_samples.append(sample)\n",
    "\n",
    "        # Process poor answers with a score of 0\n",
    "        for context in item['Poor_answers']:\n",
    "            sample = {\n",
    "                'query': question,\n",
    "                'context': context,\n",
    "                'score': 0\n",
    "            }\n",
    "            dataset_samples.append(sample)\n",
    "\n",
    "    return dataset_samples\n",
    "\n",
    "# Load data\n",
    "json_data = load_json('../data/Final_QA_pairs.json')\n",
    "\n",
    "# Prepare dataset\n",
    "cross_encoder_dataset = prepare_cross_encoder_dataset(json_data)\n",
    "\n",
    "# Save the prepared dataset\n",
    "save_json(cross_encoder_dataset, '../data/Cross_Encoder_Finetuning_Dataset.json')\n",
    "\n",
    "print(\"Dataset prepared and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset sample size: 40\n",
      "Training dataset sample size: 1560\n",
      "Datasets saved successfully.\n",
      "Test dataset preview:\n",
      "                                               query  \\\n",
      "0  A nurse is assessing a patient who has been re...   \n",
      "1  A nurse is assessing a patient who has been re...   \n",
      "2  A nurse is responding to a code blue in the em...   \n",
      "3  A nurse is responding to a code blue in the em...   \n",
      "4  A patient has been electrocuted and is now unr...   \n",
      "\n",
      "                                             context  score  \n",
      "0  (C) Chest Compressions Site of chest compressi...      1  \n",
      "1  Placement of Defibrillation Pads for Children/...      0  \n",
      "2  1.6:  OTHER  COMMON  CAUSES  OF  CARDIAC ARRES...      1  \n",
      "3  3.2: AUTOMATED EXTERNAL DEFIBRILLATORS (AEDs):...      0  \n",
      "4  (A) Ask someone to get an Automated External D...      1  \n",
      "Training dataset preview:\n",
      "                                               query  \\\n",
      "0  A nurse is assessing a patient who has been re...   \n",
      "1  A nurse is assessing a patient who has been re...   \n",
      "2  A nurse is assessing a patient who has been re...   \n",
      "3  A nurse is assessing a patient who has been re...   \n",
      "4  A nurse is assessing a patient who has been re...   \n",
      "\n",
      "                                             context  score  \n",
      "0  Based on national health statistics from the M...      0  \n",
      "1  The heart is a muscular pump located in the ce...      0  \n",
      "2  The heart muscles receive oxygen rich blood vi...      0  \n",
      "3  On an electrocardiogram (ECG), normal heart rh...      0  \n",
      "4  1.3: RISK FACTORS FOR HEART ATTACK Survival ra...      0  \n",
      "1560\n"
     ]
    }
   ],
   "source": [
    "#load from csv\n",
    "import pandas as pd\n",
    "data = pd.read_csv('../data/finetuning/ce_finetuning_dataset(41-60).csv', header=None, names=['query', 'context', 'score'])\n",
    "# Prepare the test and train datasets\n",
    "test_dataset = pd.DataFrame()\n",
    "train_dataset = pd.DataFrame()\n",
    "\n",
    "# Iterate over each group, separating test and train samples\n",
    "for _, group in data.groupby('query'):\n",
    "    positives = group[group['score'] == 1]\n",
    "    negatives = group[group['score'] == 0]\n",
    "    \n",
    "    # Ensure there is at least one positive and one negative for the test dataset\n",
    "    if not positives.empty and not negatives.empty:\n",
    "        selected_positive = positives.sample(n=1)\n",
    "        selected_negative = negatives.sample(n=1)\n",
    "        \n",
    "        # Append selected samples to the test dataset\n",
    "        test_dataset = pd.concat([test_dataset, selected_positive, selected_negative])\n",
    "\n",
    "        # Append the remaining data to the training dataset\n",
    "        # We drop the selected samples by index from the original group\n",
    "        remaining_samples = group.drop(selected_positive.index).drop(selected_negative.index)\n",
    "        train_dataset = pd.concat([train_dataset, remaining_samples])\n",
    "\n",
    "# Reset indices for clean datasets\n",
    "test_dataset.reset_index(drop=True, inplace=True)\n",
    "train_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Test dataset sample size:\", len(test_dataset))\n",
    "print(\"Training dataset sample size:\", len(train_dataset))\n",
    "# Save test and train datasets to CSV files\n",
    "test_dataset.to_csv('../data/finetuning/test_dataset.csv', index=False)\n",
    "train_dataset.to_csv('../data/finetuning/train_dataset.csv', index=False)\n",
    "\n",
    "print(\"Datasets saved successfully.\")\n",
    "print(\"Test dataset preview:\")\n",
    "print(test_dataset.head())\n",
    "print(\"Training dataset preview:\")\n",
    "print(train_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-legacy in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (0.9.48)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (3.10.2)\n",
      "Requirement already satisfied: dataclasses-json in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (2024.6.1)\n",
      "Requirement already satisfied: httpx in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (3.9)\n",
      "Requirement already satisfied: numpy in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (1.40.3)\n",
      "Requirement already satisfied: pandas in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from llama-index-legacy) (0.9.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-legacy) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-legacy) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-legacy) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-legacy) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-legacy) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-legacy) (1.9.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-legacy) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-legacy) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-legacy) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-legacy) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-legacy) (4.66.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-legacy) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-legacy) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-legacy) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-legacy) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-legacy) (1.3.1)\n",
      "Requirement already satisfied: certifi in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from httpx->llama-index-legacy) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from httpx->llama-index-legacy) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from httpx->llama-index-legacy) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-legacy) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-legacy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-legacy) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-legacy) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-legacy) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-legacy) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from pandas->llama-index-legacy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from pandas->llama-index-legacy) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from pandas->llama-index-legacy) (2024.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-legacy) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-legacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-legacy) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-legacy\n",
    "%pip install llama-index-finetuning\n",
    "%pip install llama-index-llms-openai\n",
    "!pip install huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443139bf74c4404ba359c0a5d4b74535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618d440ef4784c85918e93e674f0c680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e89171a7316416ba339f4aff5d281a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m finetuning_engine \u001b[38;5;241m=\u001b[39m CrossEncoderFinetuneEngine(\n\u001b[1;32m     16\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mfinetuning_dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Finetune the cross-encoder model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mfinetuning_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages/llama_index/legacy/finetuning/cross_encoders/cross_encoder.py:87\u001b[0m, in \u001b[0;36mCrossEncoderFinetuneEngine.finetune\u001b[0;34m(self, **train_kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinetune\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finetune model.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_output_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# CrossEncoder library's fit function does not save model when evaluator is None\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# https://github.com/UKPLab/sentence-transformers/issues/2324\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:263\u001b[0m, in \u001b[0;36mCrossEncoder.fit\u001b[0;34m(self, train_dataloader, evaluator, epochs, loss_fct, activation_fct, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar)\u001b[0m\n\u001b[1;32m    261\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    262\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss_fct(logits, labels)\n\u001b[0;32m--> 263\u001b[0m \u001b[43mloss_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_grad_norm)\n\u001b[1;32m    265\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_dataset = pd.read_csv('../data/finetuning/train_dataset.csv')\n",
    "class CrossEncoderFinetuningDatasetSample:\n",
    "    def __init__(self, query, context, score):\n",
    "        self.query = query\n",
    "        self.context = context\n",
    "        self.score = score\n",
    "finetuning_dataset = [CrossEncoderFinetuningDatasetSample(d[0], d[1], d[2]) for d in train_dataset.values]\n",
    "print(len(finetuning_dataset))\n",
    "\n",
    "from llama_index.legacy.finetuning.cross_encoders.cross_encoder import CrossEncoderFinetuneEngine\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialise the cross-encoder fine-tuning engine\n",
    "finetuning_engine = CrossEncoderFinetuneEngine(\n",
    "    dataset=finetuning_dataset, epochs=2, batch_size=8\n",
    ")\n",
    "\n",
    "# Finetune the cross-encoder model\n",
    "finetuning_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a8ebad6b874b6bbf532f36db6f4013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354dd6aaefbe4792b5c7a26a2fdf57ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b82a58ce0294e0a8013dcad20b09375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetuning_engine.push_to_hub(\n",
    "    repo_id=\"ethan-cyj/Cross-Encoder-Finetuned\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethan/Documents/GitHub/test-buddy/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be482564c9544a0a6aaf9f8f49075f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/854 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27db7b85a9e443e3b6619b0c72f575a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81b535841ee4c1e94261076b93bc562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4ffd7f333b4e6ba0e45352c2c51da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8b7ff64b75453c828bdd926af77d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3aab5c5531445b4895af25a01e95cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "base_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "finetuned_model = CrossEncoder('ethan-cyj/Cross-Encoder-Finetuned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Base Accuracy': 0.725, 'Base Precision': 1.0, 'Base Recall': 0.45, 'Base F1': 0.6206896551724138, 'Finetuned Accuracy': 0.875, 'Finetuned Precision': 0.9411764705882353, 'Finetuned Recall': 0.8, 'Finetuned F1': 0.8648648648648649}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "df = pd.read_csv('../data/finetuning/test_dataset.csv')\n",
    "for index, row in df.iterrows():\n",
    "    query = row['query']\n",
    "    context = row['context']\n",
    "    base_score = base_model.predict([(query, context)])[0]\n",
    "    finetuned_score = finetuned_model.predict([(query, context)])[0]\n",
    "    results.append({\n",
    "        'query': query,\n",
    "        'context': context,\n",
    "        'label': row['score'],\n",
    "        'base_score': base_score,\n",
    "        'finetuned_score': finetuned_score\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assume a threshold for classification\n",
    "threshold = 0.5\n",
    "results_df['base_pred'] = (results_df['base_score'] >= threshold).astype(int)\n",
    "results_df['finetuned_pred'] = (results_df['finetuned_score'] >= threshold).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'Base Accuracy': accuracy_score(results_df['label'], results_df['base_pred']),\n",
    "    'Base Precision': precision_score(results_df['label'], results_df['base_pred']),\n",
    "    'Base Recall': recall_score(results_df['label'], results_df['base_pred']),\n",
    "    'Base F1': f1_score(results_df['label'], results_df['base_pred']),\n",
    "    'Finetuned Accuracy': accuracy_score(results_df['label'], results_df['finetuned_pred']),\n",
    "    'Finetuned Precision': precision_score(results_df['label'], results_df['finetuned_pred']),\n",
    "    'Finetuned Recall': recall_score(results_df['label'], results_df['finetuned_pred']),\n",
    "    'Finetuned F1': f1_score(results_df['label'], results_df['finetuned_pred'])\n",
    "}\n",
    "\n",
    "# Display metrics\n",
    "print(metrics)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
