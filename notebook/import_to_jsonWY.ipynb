{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from groq import Groq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODIFIED CODE TO REVAMP AND RUN VIA GROQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 7-17 â€“ Unconscious Infant FBAO Relief for healthcare providers Note: 1. For healthcare providers, provide ventilations via a bag-valve-mask (BVM). If there is no BVM available and you are unable or unwilling to do mouth-to-mouth ventilations for any reason, check for normal breathing and pulse for not more than 10 seconds. 2. If no BVM is available and you are unable or unwilling to do mouth-to-mouth ventilations, perform continuous chest compressions. If you are a single rescuer and feeling tired, you may take a rest of not more than 10 seconds (preferably after 100 compressions). \n",
      "I'm ready to assist. Please provide the medical text that needs cleaning. I will expand the abbreviations, clarify the medical jargon, and return the cleaned text with the list of expansions at the end.\n",
      "\n",
      "Please go ahead and provide the text. I will format the output with line breaks (`\\n`) as needed."
     ]
    }
   ],
   "source": [
    "# Code to read one text file and transform it to a json file, just do a loop over ur own folder \n",
    "number = str(89)\n",
    "\n",
    "file_path = '../data/page_texts/6_wenyeong/page_' + number + '.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    page_content = file.read()\n",
    "    # Append the content to the list\n",
    "    print(page_content)\n",
    "\n",
    "page_dict = {'page_' + number: {}}\n",
    "page_dict['page_' + number]['Text'] = page_content\n",
    "page_dict['page_' + number]['Page'] = int(file_path.split(\"/\")[-1].split(\".\")[0][-2:])\n",
    "GROQ_API_KEY = # I AM NOT REVEALING MY API KEY :p\n",
    "client = Groq(api_key = GROQ_API_KEY)\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI assistant specializing in medical text cleaning for a Singaporean hospital's nursing manual. Your task is to expand abbreviations and clarify medical jargon. When presented with a piece of text, you will return the cleaned version with every abbreviation expanded.\\n\\nInstructions:\\n1. Expand all medical abbreviations in the text.\\n2. Maintain the original text structure and formatting.\\n3. Place expanded terms in parentheses immediately after the original abbreviation.\\n4. Do not alter medical terms that are not abbreviations.\\n5. If an abbreviation is unclear or could have multiple meanings, choose the most likely expansion based on the context.\\n6. At the end of the cleaned text, provide a list of all abbreviations you expanded and their full forms.\\n\\nExample input: \\\"The pt presented with SOB and was admitted to the ICU.\\\"\\nExample output: \\\"The pt (patient) presented with SOB (shortness of breath) and was admitted to the ICU (Intensive Care Unit).\\n\\nList of expansions:\\n- pt: patient\\n- SOB: shortness of breath\\n- ICU: Intensive Care Unit\\\"\\n\\nAs this is a JSON that I will parse, you WILL INCLUDE '\\n' in areas where there is a new line required\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "\n",
    "\n",
    "json_file_path = '../data/cleaned_pages/6_wenyeong/page_' + number + '.json'\n",
    "os.makedirs(os.path.dirname(json_file_path), exist_ok=True)\n",
    "\n",
    "# Write the dictionary to a JSON file\n",
    "with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(page_dict, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
