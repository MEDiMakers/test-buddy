{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import trange\n",
    "import time\n",
    "import re\n",
    "\n",
    "from llama_index.legacy import Document\n",
    "from llama_index.legacy.schema import TextNode\n",
    "from llama_index.legacy.node_parser import SentenceWindowNodeParser, SemanticSplitterNodeParser\n",
    "from llama_index.legacy.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.legacy.finetuning import (\n",
    "    generate_qa_embedding_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ") \n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from groq import Groq\n",
    "\n",
    "## load evv variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "GROQ_API_KEY       = os.environ[\"GROQ_API_KEY\"]\n",
    "CHAT_MODEL         = \"llama3-70b-8192\"\n",
    "client = Groq()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 16:44:50,697 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answers': [\"According to the Ministry of Health's 2019 national health statistics, ischemic heart disease accounts for approximately 18.8% of total mortality in Singapore.\",\n",
       "  'In 2019, national health statistics from the Ministry of Health revealed that ischemic heart disease was responsible for 18.8% of all deaths in Singapore.',\n",
       "  \"The Ministry of Health's 2019 data indicates that ischemic heart disease contributes to nearly one-fifth (18.8%) of total mortality in Singapore.\",\n",
       "  'National health statistics from the Ministry of Health in 2019 show that ischemic heart disease is attributed to 18.8% of total deaths in Singapore.',\n",
       "  \"As per the Ministry of Health's 2019 national health statistics, ischemic heart disease makes up 18.8% of the total mortality rate in Singapore.\"]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENERATE_SIMILAR_ANSWER_PROMPT = \\\n",
    "'''You are an expert in linguistic variation and medical communication. \n",
    "Your task is to generate 5 similar, yet distinct, answers for a given question-answer pair. \n",
    "These answers should maintain the core meaning and accuracy of the original answer but vary in phrasing, word choice, and sentence structure to provide different ways of expressing the same response. \n",
    "The variations should be natural, contextually appropriate, and reflect the kind of nuanced responses that different knowledgeable individuals might provide.\n",
    "\n",
    "For each question-answer pair provided below, generate 5 alternative answers that are consistent with the original answer's intent and content:\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Original Answer:\n",
    "{answer}\n",
    "{format_instructions}\n",
    "\n",
    "Ensure and double check that the answer is in accordance to the format above.\n",
    "'''\n",
    "\n",
    "GENERATE_POOR_ANSWER_PROMPT = \\\n",
    "'''You are an expert in generating misleading and irrelevant content. \n",
    "Your task is to produce 5 responses for a given question-answer pair that are completely opposite, incorrect, and irrelevant to the original answer.\n",
    "These responses should disregard the core meaning and accuracy of the original answer, instead focusing on providing misleading, confusing, or nonsensical information.\n",
    "The variations should be inconsistent, contextually inappropriate, and reflect the kind of responses that lack knowledge or understanding.\n",
    "\n",
    "For each question-answer pair provided below, generate 5 alternative answers that are contradictory, poorly constructed, and irrelevant to the original answer's intent and content:\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Original Answer:\n",
    "{answer}\n",
    "{format_instructions}\n",
    "\n",
    "Ensure and double-check that the generated answers are incoherent, misleading, and do not align with the original answer in any meaningful way.\n",
    "'''\n",
    "\n",
    "def extract_answer(input_string):\n",
    "    \"\"\"\n",
    "    Extracts and returns the JSON data from a given input string.\n",
    "\n",
    "    This function attempts to extract a JSON object from the provided input string.\n",
    "    The input string is expected to contain JSON data starting with `{` and ending\n",
    "    with `}`. If a valid JSON object is not found, it attempts to locate a JSON object\n",
    "    containing a 'questions' key using regular expressions.\n",
    "\n",
    "    Args:\n",
    "        input_string (str): The input string that potentially contains JSON data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary representing the extracted JSON data.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no JSON data is found in the input string.\n",
    "        json.JSONDecodeError: If the JSON data is malformed and cannot be parsed.\n",
    "\n",
    "    \"\"\"\n",
    "    # Find the start and end indices of the JSON data within the input string\n",
    "    # Assuming the JSON data starts with '{' and ends with '}'\n",
    "    json_start = input_string.find('{')\n",
    "    json_end = input_string.rfind('}') + 1\n",
    "    \n",
    "    # If either the start or end index is not found, raise an error\n",
    "    if json_start == -1 or json_end == -1:\n",
    "        raise ValueError(\"Invalid input: No JSON data found.\")\n",
    "\n",
    "    # Extract the substring that potentially contains the JSON data\n",
    "    json_data = input_string[json_start:json_end]\n",
    "    \n",
    "    try:\n",
    "        # Attempt to convert the JSON string to a Python dictionary\n",
    "        data_dict = json.loads(json_data)\n",
    "        return data_dict\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        # If JSON decoding fails, search for a JSON object containing the 'questions' key\n",
    "        # Using regex to match a pattern that includes the 'questions' key\n",
    "        pattern = r'{\\s*\"questions\":\\s*\\[.*?\\]\\s*}'\n",
    "        match = re.search(pattern, input_string, re.DOTALL)\n",
    "\n",
    "        if match:\n",
    "            # If a match is found, extract the matched JSON string and convert it to a dictionary\n",
    "            data_json_str = match.group(0)\n",
    "            data_dict = json.loads(data_json_str)\n",
    "            return data_dict\n",
    "\n",
    "        # If no valid JSON is found, the function will Log an error\n",
    "        else:\n",
    "            logging.error(\"No dictionary with 'questions' as a key found in this input string. Error by LLM\")\n",
    "            return {\"error\": \"No dictionary with questions found\"}\n",
    "\n",
    "def generate_answers(qa_pair, answer_prompt, client):\n",
    "    \"\"\"\n",
    "    Generates answers based on a list of questions and a text corpus using a language model.\n",
    "\n",
    "    This function takes in a text corpus and a list of questions, and uses a language model\n",
    "    to generate corresponding answers. The answers are formatted according to the provided\n",
    "    prompt template and are returned as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        page_text (str): The text corpus from which answers will be generated.\n",
    "        questions (str): A list of questions in json string that the model should answer based on the text.\n",
    "        answer_prompt (str): A template prompt used to instruct the model on how to generate answers.\n",
    "        client: A client object for interacting with the language model API.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the generated answers.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the 'error' key is found in the response dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    qn = qa_pair['Question']\n",
    "    ans = qa_pair['Answer']\n",
    "\n",
    "    # Define a Pydantic model for the expected answer structure\n",
    "    class AnswerList(BaseModel):\n",
    "        answers: list = Field(description=\"Similar answer to the provided answer\")\n",
    "    \n",
    "    # Initialize a JSON output parser using the defined Pydantic model\n",
    "    parser = JsonOutputParser(pydantic_object=AnswerList)\n",
    "\n",
    "    # Prepare the prompt using the provided answer prompt template, text, and list of questions\n",
    "    prompt = PromptTemplate(\n",
    "        template=answer_prompt,\n",
    "        input_variables=[\"answer\", \"question\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    ) \n",
    "    \n",
    "    # Format the final prompt with the actual text data and question list\n",
    "    final_prompt = prompt.format(question=qn, answer=ans)\n",
    "\n",
    "    # Generate the completion by interacting with the language model API\n",
    "    completion = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": final_prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,  # Control the randomness of the output (lower means less random)\n",
    "        max_tokens=1024,  # Limit the response length\n",
    "        top_p=1,  # Nucleus sampling parameter (1 means only the most likely tokens are considered)\n",
    "        stream=True,  # Enable streaming of the response chunks\n",
    "        stop=None,  # Define stopping conditions (None means no stopping condition)\n",
    "    )\n",
    "\n",
    "    # Initialize an empty string to accumulate the response content\n",
    "    answer = ''''''\n",
    "    for chunk in completion:\n",
    "        # Append each chunk of content to the answer string\n",
    "        answer += chunk.choices[0].delta.content or \"\"\n",
    "    \n",
    "    # Extract the answers from the accumulated response content\n",
    "    answer_dict = extract_answer(answer)\n",
    "\n",
    "    # Log an error if the response contains an 'error' key\n",
    "    if \"error\" in answer_dict:\n",
    "        logging.error(f\"{answer_dict['error']}\")\n",
    "    \n",
    "    # Return the dictionary containing the generated answers\n",
    "    return answer_dict\n",
    "\n",
    "with open(\"../data/final_draft_2_pairs.json\", \"r\", encoding=\"utf-8\") as fin:\n",
    "    pairs = json.load(fin)\n",
    "    \n",
    "\n",
    "for i in trange(len(pairs)):\n",
    "    sim_ans  = generate_answers(pairs[i], GENERATE_SIMILAR_ANSWER_PROMPT, client)\n",
    "    poor_ans = generate_answers(pairs[i], GENERATE_POOR_ANSWER_PROMPT, client)\n",
    "\n",
    "    pairs[i]['Similar Answers'] = sim_ans['answers']\n",
    "    pairs[i]['Poor Answers'] = sim_ans['answers']\n",
    "\n",
    "    # account for rate limit\n",
    "    if i > 0 and i % 5 == 0:\n",
    "        time.sleep(62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_buddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
