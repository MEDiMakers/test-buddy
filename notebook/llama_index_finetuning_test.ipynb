{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.legacy.finetuning.cross_encoders.dataset_gen import (\n",
    "    CrossEncoderFinetuningDatasetSample,\n",
    ")\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "from llama_index.legacy.llms import OpenAI\n",
    "import os\n",
    "from tqdm import trange\n",
    "import json\n",
    "from llama_index.legacy.schema import TextNode\n",
    "from llama_index.legacy.node_parser import SentenceWindowNodeParser, SemanticSplitterNodeParser\n",
    "from llama_index.legacy.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from groq import Groq\n",
    "\n",
    "## load env variables\n",
    "GROQ_API_KEY       = os.environ[\"GROQ_API_KEY\"]\n",
    "CHAT_MODEL         = \"llama3-70b-8192\"\n",
    "client = Groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/combined_txts/combined_output.txt\", \"r\", encoding='utf-8') as fin:\n",
    "    contexts = fin.read()\n",
    "with open(\"../data/final_draft_2_pairs.json\", \"r\", encoding=\"utf-8\") as fin:\n",
    "    pairs = json.load(fin)\n",
    "    \n",
    "sections = [section for section in contexts.split(\"\\n\\n\\n\") if section]\n",
    "questions = [pair['Question'] for pair in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_QUERY_DOC_RELEVANCE_PROMPT = \\\n",
    "'''You are an Assistant responsible for helping detect whether the retrieved document is relevant to the query. For a given input, you need to output a single token: \"Yes\" or \"No\" indicating the retrieved document is relevant to the query.\n",
    "\n",
    "Query: How to plant a tree?\n",
    "Document: \"\"\"Cars were invented in 1886, when German inventor Carl Benz patented his Benz Patent-Motorwagen.[3][4][5] Cars became widely available during the 20th century. One of the first cars affordable by the masses was the 1908 Model T, an American car manufactured by the Ford Motor Company. Cars were rapidly adopted in the US, where they replaced horse-drawn carriages.[6] In Europe and other parts of the world, demand for automobiles did not increase until after World War II.[7] The car is considered an essential part of the developed economy.\"\"\"\n",
    "Relevant: No\n",
    "\n",
    "Query: Has the coronavirus vaccine been approved?\n",
    "Document: \"\"\"The Pfizer-BioNTech COVID-19 vaccine was approved for emergency use in the United States on December 11, 2020.\"\"\"\n",
    "Relevant: Yes\n",
    "\n",
    "Query: What is the capital of France?\n",
    "Document: \"\"\"Paris, France's capital, is a major European city and a global center for art, fashion, gastronomy and culture. Its 19th-century cityscape is crisscrossed by wide boulevards and the River Seine. Beyond such landmarks as the Eiffel Tower and the 12th-century, Gothic Notre-Dame cathedral, the city is known for its cafe culture and designer boutiques along the Rue du Faubourg Saint-Honoré.\"\"\"\n",
    "Relevant: Yes\n",
    "\n",
    "Query: What are some papers to learn about PPO reinforcement learning?\n",
    "Document: \"\"\"Proximal Policy Optimization and its Dynamic Version for Sequence Generation: In sequence generation task, many works use policy gradient for model optimization to tackle the intractable backpropagation issue when maximizing the non-differentiable evaluation metrics or fooling the discriminator in adversarial learning. In this paper, we replace policy gradient with proximal policy optimization (PPO), which is a proved more efficient reinforcement learning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We demonstrate the efficacy of PPO and PPO-dynamic on conditional sequence generation tasks including synthetic experiment and chit-chat chatbot. The results show that PPO and PPO-dynamic can beat policy gradient by stability and performance.\"\"\"\n",
    "Relevant: Yes\n",
    "\n",
    "Query: Explain sentence embeddings\n",
    "Document: \"\"\"Inside the bubble: exploring the environments of reionisation-era Lyman-α emitting galaxies with JADES and FRESCO: We present a study of the environments of 16 Lyman-α emitting galaxies (LAEs) in the reionisation era (5.8<z<8) identified by JWST/NIRSpec as part of the JWST Advanced Deep Extragalactic Survey (JADES). Unless situated in sufficiently (re)ionised regions, Lyman-α emission from these galaxies would be strongly absorbed by neutral gas in the intergalactic medium (IGM). We conservatively estimate sizes of the ionised regions required to reconcile the relatively low Lyman-α velocity offsets (ΔvLyα<300kms−1) with moderately high Lyman-α escape fractions (fesc,Lyα>5%) observed in our sample of LAEs, indicating the presence of ionised ``bubbles'' with physical sizes of the order of 0.1pMpc≲Rion≲1pMpc in a patchy reionisation scenario where the bubbles are embedded in a fully neutral IGM. Around half of the LAEs in our sample are found to coincide with large-scale galaxy overdensities seen in FRESCO at z∼5.8-5.9 and z∼7.3, suggesting Lyman-α transmission is strongly enhanced in such overdense regions, and underlining the importance of LAEs as tracers of the first large-scale ionised bubbles. Considering only spectroscopically confirmed galaxies, we find our sample of UV-faint LAEs (MUV≳−20mag) and their direct neighbours are generally not able to produce the required ionised regions based on the Lyman-α transmission properties, suggesting lower-luminosity sources likely play an important role in carving out these bubbles. These observations demonstrate the combined power of JWST multi-object and slitless spectroscopy in acquiring a unique view of the early stages of Cosmic Reionisation via the most distant LAEs.\"\"\"\n",
    "Relevant: No\n",
    "\n",
    "Query: {query}\n",
    "Document: \"\"\"{document}\"\"\"\n",
    "Relevant:\n",
    "'''\n",
    "\n",
    "def generate_bool(context, question, relevance_prompt, client):\n",
    "\n",
    "    # Prepare the prompt using the provided answer prompt template, text, and list of questions\n",
    "    prompt = PromptTemplate(\n",
    "        template=relevance_prompt,\n",
    "        input_variables=[\"query\", \"document\"],\n",
    "    ) \n",
    "    \n",
    "    # Format the final prompt with the actual text data and question list\n",
    "    final_prompt = prompt.format(query=question, document=context)\n",
    "\n",
    "    # Generate the completion by interacting with the language model API\n",
    "    completion = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": final_prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,  # Control the randomness of the output (lower means less random)\n",
    "        max_tokens=1024,  # Limit the response length\n",
    "        top_p=1,  # Nucleus sampling parameter (1 means only the most likely tokens are considered)\n",
    "        stream=True,  # Enable streaming of the response chunks\n",
    "        stop=None,  # Define stopping conditions (None means no stopping condition)\n",
    "    )\n",
    "\n",
    "    # Initialize an empty string to accumulate the response content\n",
    "    answer = ''''''\n",
    "    for chunk in completion:\n",
    "        # Append each chunk of content to the answer string\n",
    "        answer += chunk.choices[0].delta.content or \"\"\n",
    "    \n",
    "    # Return the dictionary containing the generated answers\n",
    "    return answer\n",
    "\n",
    "# Usage:\n",
    "# ans = generate_bool(contexts[1], questions[1], DEFAULT_QUERY_DOC_RELEVANCE_PROMPT, client)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted and modified from llama index open source library\n",
    "def generate_ce_fine_tuning_dataset(contexts, questions_list, qa_doc_relevance_prompt):\n",
    "    cross_encoder_dataset_list = []\n",
    "    \n",
    "    for question in trange(questions_list):\n",
    "        if question != \"\":\n",
    "            i = 0\n",
    "            for context in contexts:\n",
    "                # Generates the Yes or No for each question document pair \n",
    "                response = generate_bool(context, question, qa_doc_relevance_prompt, client)\n",
    "                # Lowercase the response\n",
    "                result = response.lower()\n",
    "                \n",
    "                if result == \"yes\":\n",
    "                    question_row = CrossEncoderFinetuningDatasetSample(\n",
    "                        query=question, context=context, score=1\n",
    "                    )\n",
    "                    cross_encoder_dataset_list.append(question_row)\n",
    "                    \n",
    "                elif result == \"no\":\n",
    "                    question_row = CrossEncoderFinetuningDatasetSample(\n",
    "                        query=question, context=context, score=0\n",
    "                    )\n",
    "                    cross_encoder_dataset_list.append(question_row)\n",
    "                else:\n",
    "                    logging.error(\"Error in LLM output... Edit prompt\")\n",
    "                    \n",
    "            i += 1\n",
    "            if i >0 and i % 11 == 0:\n",
    "                print(\"Sleeping now...\")\n",
    "                time.sleep(62)\n",
    "    return cross_encoder_dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ce_fine_tuning_dataset(contexts[:5], questions[:5], DEFAULT_QUERY_DOC_RELEVANCE_PROMPT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_buddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
