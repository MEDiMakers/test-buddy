{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import trange\n",
    "import time\n",
    "import re\n",
    "\n",
    "from llama_index.legacy import Document\n",
    "from llama_index.legacy.schema import TextNode\n",
    "from llama_index.legacy.node_parser import SentenceWindowNodeParser, SemanticSplitterNodeParser\n",
    "from llama_index.legacy.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.legacy.finetuning import (\n",
    "    generate_qa_embedding_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ") \n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "## load evv variables\n",
    "HF_KEY             = os.environ[\"HUGGINGFACE_API_KEY\"]\n",
    "OPENAI_KEY         = os.environ[\"OPENAI_API_KEY\"]\n",
    "GROQ_API_KEY       = os.environ[\"GROQ_API_KEY\"]\n",
    "CHAT_MODEL         = \"llama3-70b-8192\"\n",
    "client = Groq()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the files\n",
    "def export_text():\n",
    "    all_txts = os.listdir('../data/combined_txts')\n",
    "\n",
    "    all_sections = []\n",
    "\n",
    "    for i in range(len(all_txts)):\n",
    "        with open(f\"../data/combined_txts/{all_txts[i]}\", \"r\", encoding='utf-8') as fin:\n",
    "            text = fin.read()\n",
    "            sections = text.split(\"\\n\\n\\n\")\n",
    "            all_sections.extend(sections)\n",
    "            \n",
    "    with open(\"../data/all_pdf_text.json\", \"w\", encoding='utf-8') as fout:\n",
    "        json.dump(all_sections, fout, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating QA Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_QUESTION_PROMPT = \\\n",
    "'''You are a professor proficient in medical aid. You are tasked with generating questions for a test about the content of the text corpus of medical information from a manual made for hospital nurses in Singapore.\n",
    "You are to only refer to the text corpus below for the generation of questions. \n",
    "For the text corpus below, generate 5 high quality test questions for an upcoming examination for hospital nurses. \n",
    "Do not provide the answers.\n",
    "\n",
    "Text corpus:\n",
    "{text}\n",
    "{format_instructions}\n",
    "Ensure and double check that the answer is in accordance to the format above.\n",
    "'''\n",
    "\n",
    "GENERATE_ANSWER_PROMPT = \\\n",
    "'''You are a professor proficient in medical aid. You are tasked with generating answers for a test about the content of the text corpus of medical information from a manual made for hospital nurses in Singapore.\n",
    "You are to only refer to the text corpus below for the answering of questions. \n",
    "For the text corpus below, generate 5 high quality and well elaborated answers for each of the questions.\n",
    "\n",
    "Text Corpus:\n",
    "{text}\n",
    "\n",
    "List of questions:\n",
    "{question_list}\n",
    "{format_instructions}\n",
    "\n",
    "Ensure and double check that the answer is in accordance to the format above.\n",
    "'''\n",
    "\n",
    "def extract_answer(input_string):\n",
    "    # Trim the extraneous part of the string if necessary\n",
    "    # Assuming the JSON data starts with `{` and ends with `}`\n",
    "    json_start = input_string.find('{')\n",
    "    json_end = input_string.rfind('}') + 1\n",
    "    \n",
    "    if json_start == -1 or json_end == -1:\n",
    "        raise ValueError(\"Invalid input: No JSON data found.\")\n",
    "\n",
    "    json_data = input_string[json_start:json_end]\n",
    "    \n",
    "    try:\n",
    "        # Convert the JSON string to a Python dictionary\n",
    "        data_dict = json.loads(json_data)\n",
    "        return data_dict\n",
    "    \n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        # Use regex to find the JSON object with the 'questions' key\n",
    "        pattern = r'{\\s*\"questions\":\\s*\\[.*?\\]\\s*}'\n",
    "        match = re.search(pattern, input_string, re.DOTALL)\n",
    "\n",
    "        if match:\n",
    "            data_json_str = match.group(0)\n",
    "            data_dict = json.loads(data_json_str)\n",
    "            return data_dict\n",
    "\n",
    "def generate_questions(page_text, question_prompt, client):\n",
    "    class  questions(BaseModel):\n",
    "            questions: str = Field(description=\"Question about the content in the text corpus\")\n",
    "            \n",
    "    parser = JsonOutputParser(pydantic_object= questions)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "            template=question_prompt,\n",
    "            input_variables=[\"text\"],\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        ) \n",
    "    \n",
    "    final_prompt = prompt.format(text=page_text)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "            model=CHAT_MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": final_prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=1024,\n",
    "            top_p=1,\n",
    "            stream=True,\n",
    "            stop=None,\n",
    "        )\n",
    "\n",
    "    answer = ''''''\n",
    "    for chunk in completion:\n",
    "        answer += chunk.choices[0].delta.content or \"\"\n",
    "    question_dict = extract_answer(answer)\n",
    "    if \"error\" in question_dict:\n",
    "        logging.error(f\"{question_dict['error']}\")\n",
    "    return question_dict\n",
    "    \n",
    "def generate_answers(page_text, questions, answer_prompt, client):\n",
    "    class answer_list(BaseModel):\n",
    "        answers: list = Field(description=\"Answer to the question with reference to the content in the text corpus\")\n",
    "            \n",
    "    parser = JsonOutputParser(pydantic_object=answer_list)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "            template=answer_prompt,\n",
    "            input_variables=[\"text\", \"question_list\"],\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        ) \n",
    "    \n",
    "    final_prompt = prompt.format(text = page_text, question_list=questions)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "            model=CHAT_MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": final_prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=1024,\n",
    "            top_p=1,\n",
    "            stream=True,\n",
    "            stop=None,\n",
    "        )\n",
    "\n",
    "    answer = ''''''\n",
    "    for chunk in completion:\n",
    "        answer += chunk.choices[0].delta.content or \"\"\n",
    "    answer_dict = extract_answer(answer)\n",
    "    if \"error\" in answer_dict:\n",
    "        logging.error(f\"{answer_dict['error']}\")\n",
    "    return answer_dict\n",
    "\n",
    "def generate_section_QA_pairs(section_text, client, question_prompt, answer_prompt):\n",
    "    question_dict = generate_questions(section_text, question_prompt, client)\n",
    "    question_list = [q_pair for q_pair in question_dict['questions']]\n",
    "    question_str = json.dumps(question_list)\n",
    "    answer_dict = generate_answers(section_text, question_str, answer_prompt, client)\n",
    "    answer_list = answer_dict[\"answers\"]\n",
    "    question_list = question_dict['questions']\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for i in range(len(question_list)):\n",
    "        qa_pairs.append({\"Question\": question_list[i], \"Answer\": answer_list[i]})\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "def generate_all_qa_pairs():\n",
    "    with open(\"../data/all_pdf_text.json\", \"r\", encoding=\"utf-8\") as fin:\n",
    "        all_sections = json.load(fin)\n",
    "    \n",
    "    all_pairs = []\n",
    "    for i in trange(len(all_sections)):\n",
    "        section_qa_pair = generate_section_QA_pairs(all_sections[i], client, GENERATE_QUESTION_PROMPT, GENERATE_ANSWER_PROMPT)\n",
    "        all_pairs.extend(section_qa_pair)\n",
    "        \n",
    "        with open(\"../data/QA_pairs.json\", \"w\", encoding='utf-8') as fout:\n",
    "            json.dump(all_pairs, fout, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        # Ensure API rate limit does not exceed\n",
    "        if i>29 and i % 30 == 0:\n",
    "            time.sleep(65)\n",
    "        if i >2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]2024-08-14 15:51:40,724 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-14 15:51:41,591 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  2%|▏         | 1/64 [00:02<02:51,  2.72s/it]2024-08-14 15:51:43,456 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-14 15:51:44,063 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  3%|▎         | 2/64 [00:04<02:20,  2.26s/it]2024-08-14 15:51:45,326 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-14 15:51:46,148 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  5%|▍         | 3/64 [00:06<01:59,  1.96s/it]2024-08-14 15:51:46,927 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-14 15:51:47,893 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  5%|▍         | 3/64 [00:09<03:08,  3.09s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_all_qa_pairs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
