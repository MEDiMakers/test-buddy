{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import trange\n",
    "import time\n",
    "import re\n",
    "\n",
    "from llama_index.legacy import Document\n",
    "from llama_index.legacy.schema import TextNode\n",
    "from llama_index.legacy.node_parser import SentenceWindowNodeParser, SemanticSplitterNodeParser\n",
    "from llama_index.legacy.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.legacy.finetuning import (\n",
    "    generate_qa_embedding_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ") \n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "## load env variables\n",
    "HF_KEY             = os.environ[\"HUGGINGFACE_API_KEY\"]\n",
    "OPENAI_KEY         = os.environ[\"OPENAI_API_KEY\"]\n",
    "GROQ_API_KEY       = os.environ[\"GROQ_API_KEY\"]\n",
    "CHAT_MODEL         = \"llama3-70b-8192\"\n",
    "client = Groq()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the files\n",
    "def export_text():\n",
    "    all_txts = os.listdir('../data/combined_txts')\n",
    "\n",
    "    all_sections = []\n",
    "\n",
    "    for i in range(len(all_txts)):\n",
    "        with open(f\"../data/combined_txts/{all_txts[i]}\", \"r\", encoding='utf-8') as fin:\n",
    "            text = fin.read()\n",
    "            sections = text.split(\"\\n\\n\\n\")\n",
    "            all_sections.extend(sections)\n",
    "            \n",
    "    with open(\"../data/all_pdf_text.json\", \"w\", encoding='utf-8') as fout:\n",
    "        json.dump(all_sections, fout, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating QA Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_QUESTION_PROMPT = \\\n",
    "'''You are a professor proficient in medical aid. You are tasked with generating questions for a test about the content of the text corpus of medical information from a manual made for hospital nurses in Singapore.\n",
    "You are to only refer to the text corpus below for the generation of questions. \n",
    "For the text corpus below, generate 5 high quality test questions for an upcoming examination for hospital nurses. \n",
    "Do not provide the answers.\n",
    "\n",
    "Text corpus:\n",
    "{text}\n",
    "{format_instructions}\n",
    "Ensure and double check that the answer is in accordance to the format above.\n",
    "'''\n",
    "\n",
    "GENERATE_ANSWER_PROMPT = \\\n",
    "'''You are a professor proficient in medical aid. You are tasked with generating answers for a test about the content of the text corpus of medical information from a manual made for hospital nurses in Singapore.\n",
    "You are to only refer to the text corpus below for the answering of questions. \n",
    "For the text corpus below, generate 5 high quality and well elaborated answers for each of the questions.\n",
    "\n",
    "Text Corpus:\n",
    "{text}\n",
    "\n",
    "List of questions:\n",
    "{question_list}\n",
    "{format_instructions}\n",
    "\n",
    "Ensure and double check that the answer is in accordance to the format above.\n",
    "'''\n",
    "\n",
    "def extract_answer(input_string):\n",
    "    # Trim the extraneous part of the string if necessary\n",
    "    # Assuming the JSON data starts with `{` and ends with `}`\n",
    "    json_start = input_string.find('{')\n",
    "    json_end = input_string.rfind('}') + 1\n",
    "    \n",
    "    if json_start == -1 or json_end == -1:\n",
    "        raise ValueError(\"Invalid input: No JSON data found.\")\n",
    "\n",
    "    json_data = input_string[json_start:json_end]\n",
    "    \n",
    "    try:\n",
    "        # Convert the JSON string to a Python dictionary\n",
    "        data_dict = json.loads(json_data)\n",
    "        return data_dict\n",
    "    \n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        # Use regex to find the JSON object with the 'questions' key\n",
    "        pattern = r'{\\s*\"questions\":\\s*\\[.*?\\]\\s*}'\n",
    "        match = re.search(pattern, input_string, re.DOTALL)\n",
    "\n",
    "        if match:\n",
    "            data_json_str = match.group(0)\n",
    "            data_dict = json.loads(data_json_str)\n",
    "            return data_dict\n",
    "\n",
    "def generate_questions(page_text, question_prompt, client):\n",
    "    class  questions(BaseModel):\n",
    "            questions: str = Field(description=\"Question about the content in the text corpus\")\n",
    "            \n",
    "    parser = JsonOutputParser(pydantic_object= questions)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "            template=question_prompt,\n",
    "            input_variables=[\"text\"],\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        ) \n",
    "    \n",
    "    final_prompt = prompt.format(text=page_text)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "            model=CHAT_MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": final_prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=1024,\n",
    "            top_p=1,\n",
    "            stream=True,\n",
    "            stop=None,\n",
    "        )\n",
    "\n",
    "    answer = ''''''\n",
    "    for chunk in completion:\n",
    "        answer += chunk.choices[0].delta.content or \"\"\n",
    "    question_dict = extract_answer(answer)\n",
    "    if \"error\" in question_dict:\n",
    "        logging.error(f\"{question_dict['error']}\")\n",
    "    return question_dict\n",
    "    \n",
    "def generate_answers(page_text, questions, answer_prompt, client):\n",
    "    class answer_list(BaseModel):\n",
    "        answers: list = Field(description=\"Answer to the question with reference to the content in the text corpus\")\n",
    "            \n",
    "    parser = JsonOutputParser(pydantic_object=answer_list)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "            template=answer_prompt,\n",
    "            input_variables=[\"text\", \"question_list\"],\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        ) \n",
    "    \n",
    "    final_prompt = prompt.format(text = page_text, question_list=questions)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "            model=CHAT_MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": final_prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=1024,\n",
    "            top_p=1,\n",
    "            stream=True,\n",
    "            stop=None,\n",
    "        )\n",
    "\n",
    "    answer = ''''''\n",
    "    for chunk in completion:\n",
    "        answer += chunk.choices[0].delta.content or \"\"\n",
    "    answer_dict = extract_answer(answer)\n",
    "    if \"error\" in answer_dict:\n",
    "        logging.error(f\"{answer_dict['error']}\")\n",
    "    return answer_dict\n",
    "\n",
    "def generate_section_QA_pairs(section_text, client, question_prompt, answer_prompt):\n",
    "    question_dict = generate_questions(section_text, question_prompt, client)\n",
    "    question_list = [q_pair for q_pair in question_dict['questions']]\n",
    "    question_str = json.dumps(question_list)\n",
    "    answer_dict = generate_answers(section_text, question_str, answer_prompt, client)\n",
    "    answer_list = answer_dict[\"answers\"]\n",
    "    question_list = question_dict['questions']\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for i in range(len(question_list)):\n",
    "        qa_pairs.append({\"Question\": question_list[i], \"Answer\": answer_list[i]})\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "def generate_all_qa_pairs():\n",
    "    with open(\"../data/all_pdf_text.json\", \"r\", encoding=\"utf-8\") as fin:\n",
    "        all_sections = json.load(fin)\n",
    "    \n",
    "    all_pairs = []\n",
    "    for i in trange(len(all_sections)):\n",
    "        section_qa_pair = generate_section_QA_pairs(all_sections[i], client, GENERATE_QUESTION_PROMPT, GENERATE_ANSWER_PROMPT)\n",
    "        all_pairs.extend(section_qa_pair)\n",
    "        \n",
    "        with open(\"../data/QA_pairs.json\", \"w\", encoding='utf-8') as fout:\n",
    "            json.dump(all_pairs, fout, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        # Ensure API rate limit does not exceed\n",
    "        if i>29 and i % 30 == 0:\n",
    "            time.sleep(65)\n",
    "        if i >2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]2024-08-14 15:51:40,724 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-14 15:51:41,591 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  2%|▏         | 1/64 [00:02<02:51,  2.72s/it]2024-08-14 15:51:43,456 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-14 15:51:44,063 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  3%|▎         | 2/64 [00:04<02:20,  2.26s/it]2024-08-14 15:51:45,326 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-14 15:51:46,148 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  5%|▍         | 3/64 [00:06<01:59,  1.96s/it]2024-08-14 15:51:46,927 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-14 15:51:47,893 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  5%|▍         | 3/64 [00:09<03:08,  3.09s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_all_qa_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70 [00:00<?, ?it/s]2024-08-18 11:41:09,012 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': ['What is the percentage of total mortality contributed by ischemic heart disease in Singapore, according to national health statistics from the Ministry of Health in 2019?', 'What is the crude incidence rate of Out-of-Hospital Cardiac Arrest (OHCA) in Singapore per 100,000 population in 2019?', 'What percentage of cardiac arrests occur in the home, according to a study conducted in Singapore?', 'What is the percentage of casualties who survived to be discharged with good-to-moderate neurological functions after receiving bystander CPR and defibrillation?', 'What is the essential requirement for performing basic life-saving skills of cardio-pulmonary resuscitation (CPR) and using automated external defibrillators (AEDs)?']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 11:41:09,931 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  1%|▏         | 1/70 [00:02<02:22,  2.06s/it]2024-08-18 11:41:11,035 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': ['What is the location of the heart in the chest?', 'What is the function of the right side of the heart?', 'Through which blood vessels does the left side of the heart receive oxygen-rich blood from the lungs?', 'What are the vital organs that receive oxygen-rich blood from the left side of the heart?', 'What is the purpose of the heart pumping blood to the lungs via the pulmonary arteries?']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 11:41:11,897 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  3%|▎         | 2/70 [00:04<02:42,  2.39s/it]2024-08-18 11:41:13,626 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': ['What is the primary source of oxygen-rich blood for the heart muscles?', 'What is the name of the node that initiates the pumping action of the heart?', 'What is the normal range of heartbeats per minute in a healthy individual?', 'What medical device can detect the electrical signals from the heart?', 'What is the term for the network that allows electrical signals to travel through the heart in an orderly manner?']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 11:41:14,315 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  4%|▍         | 3/70 [00:06<02:33,  2.29s/it]2024-08-18 11:41:15,829 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': ['What is the normal heart rate in beats per minute, as indicated by a Normal Sinus Rhythm on an electrocardiogram (ECG)?', 'What percentage of oxygen is extracted by the lungs from the air breathed in?', 'How many large squares on an ECG represent 1 second?', 'What is the purpose of the pulmonary vein in the process of oxygen exchange in the lungs?', 'What percentage of oxygen remains unabsorbed and is breathed out?']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 11:41:16,906 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  6%|▌         | 4/70 [00:08<02:20,  2.13s/it]2024-08-18 11:41:17,725 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': ['What is the primary reason why smoking increases the risk of heart attack?', 'What is the recommended action for individuals with high blood pressure to reduce their risk of heart attack?', 'What type of diet should individuals with diabetes adopt to control their blood sugar levels?', 'What is the benefit of regular exercise in managing blood lipids?', 'What is the overall effect of adopting healthy lifestyles on the risk of heart disease and other illnesses?']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 11:41:18,358 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-18 11:41:18,361 - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds\n",
      "2024-08-18 11:41:23,681 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  7%|▋         | 5/70 [00:15<04:17,  3.96s/it]2024-08-18 11:41:24,914 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-18 11:41:24,917 - INFO - Retrying request to /openai/v1/chat/completions in 8.000000 seconds\n",
      "2024-08-18 11:41:33,497 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': ['What is the main cause of a heart attack, according to the text?', 'What is the consequence of a blocked coronary artery, as described in the text?', 'Which of the following symptoms is NOT typically associated with a heart attack, according to the text?', 'What is the recommended course of action if someone experiences symptoms of a heart attack, according to the text?', 'What is the potential consequence if a heart attack is not treated promptly, as stated in the text?']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 11:41:34,544 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-18 11:41:34,545 - INFO - Retrying request to /openai/v1/chat/completions in 8.000000 seconds\n",
      "2024-08-18 11:41:42,851 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  9%|▊         | 6/70 [00:35<09:49,  9.21s/it]2024-08-18 11:41:44,339 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-08-18 11:41:44,342 - INFO - Retrying request to /openai/v1/chat/completions in 5.000000 seconds\n",
      "2024-08-18 11:41:49,644 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-08-18 11:41:50,391 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': ['What is the irregular, chaotic electrical rhythm that develops in many cases when a portion of the heart muscles dies?', 'What is the state of the casualty when Ventricular Fibrillation (VF) occurs?', 'According to Figure 1-13, what is the likely outcome if breathing stops and the heart stops beating for more than 10 minutes?', 'What is the timeframe in which brain damage is possible after breathing stops?', 'What is the term used to describe the situation when the heart does not pump blood to the rest of the body due to disrupted electrical impulses?']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 11:41:50,394 - INFO - Retrying request to /openai/v1/chat/completions in 7.000000 seconds\n",
      "  9%|▊         | 6/70 [00:46<08:18,  7.79s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/test_buddy/test_buddy/lib/python3.10/site-packages/groq/_base_client.py:997\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 997\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/test_buddy/test_buddy/lib/python3.10/site-packages/httpx/_models.py:761\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 761\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 340\u001b[0m\n\u001b[1;32m    336\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m62\u001b[39m)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[43mgenerate_all_qa_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 325\u001b[0m, in \u001b[0;36mgenerate_all_qa_pairs\u001b[0;34m()\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# Iterate over each section of text to generate QA pairs\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mlen\u001b[39m(all_sections)):\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;66;03m# Generate QA pairs for the current section\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m     section_qa_pair \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_section_QA_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_sections\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGENERATE_QUESTION_PROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGENERATE_ANSWER_PROMPT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# Extend the list of all QA pairs with the newly generated pairs\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     all_pairs\u001b[38;5;241m.\u001b[39mextend(section_qa_pair)\n",
      "Cell \u001b[0;32mIn[2], line 284\u001b[0m, in \u001b[0;36mgenerate_section_QA_pairs\u001b[0;34m(section_text, client, question_prompt, answer_prompt)\u001b[0m\n\u001b[1;32m    281\u001b[0m question_str \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(question_list)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Generate answers corresponding to the questions using the answer prompt\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m answer_dict \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43msection_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Extract the list of answers from the generated answer dictionary\u001b[39;00m\n\u001b[1;32m    287\u001b[0m answer_list \u001b[38;5;241m=\u001b[39m answer_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[2], line 224\u001b[0m, in \u001b[0;36mgenerate_answers\u001b[0;34m(page_text, questions, answer_prompt, client)\u001b[0m\n\u001b[1;32m    221\u001b[0m final_prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mpage_text, question_list\u001b[38;5;241m=\u001b[39mquestions)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Generate the completion by interacting with the language model API\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHAT_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_prompt\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Control the randomness of the output (lower means less random)\u001b[39;49;00m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Limit the response length\u001b[39;49;00m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Nucleus sampling parameter (1 means only the most likely tokens are considered)\u001b[39;49;00m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Enable streaming of the response chunks\u001b[39;49;00m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Define stopping conditions (None means no stopping condition)\u001b[39;49;00m\n\u001b[1;32m    237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Initialize an empty string to accumulate the response content\u001b[39;00m\n\u001b[1;32m    240\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124m'''\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/test_buddy/test_buddy/lib/python3.10/site-packages/groq/resources/chat/completions.py:289\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    178\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/test_buddy/test_buddy/lib/python3.10/site-packages/groq/_base_client.py:1225\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1213\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1222\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1223\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1224\u001b[0m     )\n\u001b[0;32m-> 1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/test_buddy/test_buddy/lib/python3.10/site-packages/groq/_base_client.py:920\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    913\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    918\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    919\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/test_buddy/test_buddy/lib/python3.10/site-packages/groq/_base_client.py:1003\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1002\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Desktop/test_buddy/test_buddy/lib/python3.10/site-packages/groq/_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m-> 1049\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1052\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1053\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1056\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1057\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import trange\n",
    "import time\n",
    "import re\n",
    "\n",
    "from llama_index.legacy import Document\n",
    "from llama_index.legacy.schema import TextNode\n",
    "from llama_index.legacy.node_parser import SentenceWindowNodeParser, SemanticSplitterNodeParser\n",
    "from llama_index.legacy.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.legacy.finetuning import (\n",
    "    generate_qa_embedding_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ") \n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from groq import Groq\n",
    "\n",
    "## load evv variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "GROQ_API_KEY       = os.environ[\"GROQ_API_KEY\"]\n",
    "CHAT_MODEL         = \"llama3-70b-8192\"\n",
    "client = Groq()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "GENERATE_QUESTION_PROMPT = \\\n",
    "'''You are a professor proficient in medical aid. You are tasked with generating questions for a test about the content of the text corpus of medical information from a manual made for hospital nurses in Singapore.\n",
    "You are to only refer to the text corpus below for the generation of questions. \n",
    "For the text corpus below, generate 5 high quality test questions for an upcoming examination for hospital nurses. \n",
    "Do not provide the answers.\n",
    "\n",
    "Text corpus:\n",
    "{text}\n",
    "{format_instructions}\n",
    "Ensure and double check that the answer is in accordance to the format above.\n",
    "'''\n",
    "\n",
    "GENERATE_ANSWER_PROMPT = \\\n",
    "'''You are a professor proficient in medical aid. You are tasked with generating answers for a test about the content of the text corpus of medical information from a manual made for hospital nurses in Singapore.\n",
    "You are to only refer to the text corpus below for the answering of questions. \n",
    "For the text corpus below, generate 5 high quality and well elaborated answers for each of the questions.\n",
    "\n",
    "Text Corpus:\n",
    "{text}\n",
    "\n",
    "List of questions:\n",
    "{question_list}\n",
    "{format_instructions}\n",
    "\n",
    "Ensure and double check that the answer is in accordance to the format above.\n",
    "'''\n",
    "\n",
    "\n",
    "def extract_answer(input_string):\n",
    "    \"\"\"\n",
    "    Extracts and returns the JSON data from a given input string.\n",
    "\n",
    "    This function attempts to extract a JSON object from the provided input string.\n",
    "    The input string is expected to contain JSON data starting with `{` and ending\n",
    "    with `}`. If a valid JSON object is not found, it attempts to locate a JSON object\n",
    "    containing a 'questions' key using regular expressions.\n",
    "\n",
    "    Args:\n",
    "        input_string (str): The input string that potentially contains JSON data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary representing the extracted JSON data.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no JSON data is found in the input string.\n",
    "        json.JSONDecodeError: If the JSON data is malformed and cannot be parsed.\n",
    "\n",
    "    \"\"\"\n",
    "    # Find the start and end indices of the JSON data within the input string\n",
    "    # Assuming the JSON data starts with '{' and ends with '}'\n",
    "    json_start = input_string.find('{')\n",
    "    json_end = input_string.rfind('}') + 1\n",
    "    \n",
    "    # If either the start or end index is not found, raise an error\n",
    "    if json_start == -1 or json_end == -1:\n",
    "        raise ValueError(\"Invalid input: No JSON data found.\")\n",
    "\n",
    "    # Extract the substring that potentially contains the JSON data\n",
    "    json_data = input_string[json_start:json_end]\n",
    "    \n",
    "    try:\n",
    "        # Attempt to convert the JSON string to a Python dictionary\n",
    "        data_dict = json.loads(json_data)\n",
    "        return data_dict\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        # If JSON decoding fails, search for a JSON object containing the 'questions' key\n",
    "        # Using regex to match a pattern that includes the 'questions' key\n",
    "        pattern = r'{\\s*\"questions\":\\s*\\[.*?\\]\\s*}'\n",
    "        match = re.search(pattern, input_string, re.DOTALL)\n",
    "\n",
    "        if match:\n",
    "            # If a match is found, extract the matched JSON string and convert it to a dictionary\n",
    "            data_json_str = match.group(0)\n",
    "            data_dict = json.loads(data_json_str)\n",
    "            return data_dict\n",
    "\n",
    "        # If no valid JSON is found, the function will Log an error\n",
    "        else:\n",
    "            logging.error(\"No dictionary with 'questions' as a key found in this input string. Error by LLM\")\n",
    "            return {\"error\": \"No dictionary with questions found\"}\n",
    "\n",
    "\n",
    "def generate_questions(page_text, question_prompt, client):\n",
    "    \"\"\"\n",
    "    Generates questions from a given text corpus using a language model.\n",
    "\n",
    "    This function takes in a text corpus and a question prompt template, then utilizes a\n",
    "    language model to generate questions based on the content of the text. The generated\n",
    "    questions are returned as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        page_text (str): The text corpus from which questions will be generated.\n",
    "        question_prompt (str): A template prompt used to instruct the model on how to generate questions.\n",
    "        client: A client object for interacting with the language model API.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the generated questions.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the 'error' key is found in the response dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a Pydantic model for the expected question structure\n",
    "    class Questions(BaseModel):\n",
    "        questions: str = Field(description=\"Question about the content in the text corpus\")\n",
    "    \n",
    "    # Initialize a JSON output parser using the defined Pydantic model\n",
    "    parser = JsonOutputParser(pydantic_object=Questions)\n",
    "\n",
    "    # Prepare the prompt using the provided question prompt template and input text\n",
    "    prompt = PromptTemplate(\n",
    "        template=question_prompt,\n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    ) \n",
    "    \n",
    "    # Format the final prompt with the actual text data\n",
    "    final_prompt = prompt.format(text=page_text)\n",
    "\n",
    "    # Generate the completion by interacting with the language model API\n",
    "    completion = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": final_prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,  # Control the randomness of the output (lower means less random)\n",
    "        max_tokens=1024,  # Limit the response length\n",
    "        top_p=1,  # Nucleus sampling parameter (1 means only the most likely tokens are considered)\n",
    "        stream=True,  # Enable streaming of the response chunks\n",
    "        stop=None,  # Define stopping conditions (None means no stopping condition)\n",
    "    )\n",
    "\n",
    "    # Initialize an empty string to accumulate the response content\n",
    "    answer = ''''''\n",
    "    for chunk in completion:\n",
    "        # Append each chunk of content to the answer string\n",
    "        answer += chunk.choices[0].delta.content or \"\"\n",
    "    \n",
    "    # Extract the questions from the accumulated response content\n",
    "    question_dict = extract_answer(answer)\n",
    "\n",
    "    # Log an error if the response contains an 'error' key\n",
    "    if \"error\" in question_dict:\n",
    "        logging.error(f\"{question_dict['error']}\")\n",
    "    \n",
    "    # Return the dictionary containing the generated questions\n",
    "    return question_dict\n",
    "    \n",
    "    \n",
    "def generate_answers(page_text, questions, answer_prompt, client):\n",
    "    \"\"\"\n",
    "    Generates answers based on a list of questions and a text corpus using a language model.\n",
    "\n",
    "    This function takes in a text corpus and a list of questions, and uses a language model\n",
    "    to generate corresponding answers. The answers are formatted according to the provided\n",
    "    prompt template and are returned as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        page_text (str): The text corpus from which answers will be generated.\n",
    "        questions (str): A list of questions in json string that the model should answer based on the text.\n",
    "        answer_prompt (str): A template prompt used to instruct the model on how to generate answers.\n",
    "        client: A client object for interacting with the language model API.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the generated answers.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the 'error' key is found in the response dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a Pydantic model for the expected answer structure\n",
    "    class AnswerList(BaseModel):\n",
    "        answers: list = Field(description=\"Answer to the question with reference to the content in the text corpus\")\n",
    "    \n",
    "    # Initialize a JSON output parser using the defined Pydantic model\n",
    "    parser = JsonOutputParser(pydantic_object=AnswerList)\n",
    "\n",
    "    # Prepare the prompt using the provided answer prompt template, text, and list of questions\n",
    "    prompt = PromptTemplate(\n",
    "        template=answer_prompt,\n",
    "        input_variables=[\"text\", \"question_list\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    ) \n",
    "    \n",
    "    # Format the final prompt with the actual text data and question list\n",
    "    final_prompt = prompt.format(text=page_text, question_list=questions)\n",
    "\n",
    "    # Generate the completion by interacting with the language model API\n",
    "    completion = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": final_prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,  # Control the randomness of the output (lower means less random)\n",
    "        max_tokens=1024,  # Limit the response length\n",
    "        top_p=1,  # Nucleus sampling parameter (1 means only the most likely tokens are considered)\n",
    "        stream=True,  # Enable streaming of the response chunks\n",
    "        stop=None,  # Define stopping conditions (None means no stopping condition)\n",
    "    )\n",
    "\n",
    "    # Initialize an empty string to accumulate the response content\n",
    "    answer = ''''''\n",
    "    for chunk in completion:\n",
    "        # Append each chunk of content to the answer string\n",
    "        answer += chunk.choices[0].delta.content or \"\"\n",
    "    \n",
    "    # Extract the answers from the accumulated response content\n",
    "    answer_dict = extract_answer(answer)\n",
    "\n",
    "    # Log an error if the response contains an 'error' key\n",
    "    if \"error\" in answer_dict:\n",
    "        logging.error(f\"{answer_dict['error']}\")\n",
    "    \n",
    "    # Return the dictionary containing the generated answers\n",
    "    return answer_dict\n",
    "\n",
    "\n",
    "def generate_section_QA_pairs(section_text, client, question_prompt, answer_prompt):\n",
    "    \"\"\"\n",
    "    Generates question-answer (QA) pairs from a given section of text using a language model.\n",
    "\n",
    "    This function first generates questions based on the input section of text, then uses the\n",
    "    generated questions to produce corresponding answers. The questions and answers are paired\n",
    "    together and returned as a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        section_text (str): The text from which questions and answers will be generated.\n",
    "        client: A client object for interacting with the language model API.\n",
    "        question_prompt (str): A template prompt used to instruct the model on how to generate questions.\n",
    "        answer_prompt (str): A template prompt used to instruct the model on how to generate answers.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing a question and its corresponding answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate questions from the provided text section using the question prompt\n",
    "    question_dict = generate_questions(section_text, question_prompt, client)\n",
    "    # Extract the list of questions from the generated question dictionary\n",
    "    question_list = [q_pair for q_pair in question_dict['questions']]\n",
    "\n",
    "    # Convert the list of questions to a JSON-formatted string\n",
    "    question_str = json.dumps(question_list)\n",
    "\n",
    "    # Generate answers corresponding to the questions using the answer prompt\n",
    "    answer_dict = generate_answers(section_text, question_str, answer_prompt, client)\n",
    "\n",
    "    # Extract the list of answers from the generated answer dictionary\n",
    "    answer_list = answer_dict[\"answers\"]\n",
    "\n",
    "    # Extract the list of questions again for pairing with answers\n",
    "    question_list = question_dict['questions']\n",
    "\n",
    "    # Initialize an empty list to store the QA pairs\n",
    "    qa_pairs = []\n",
    "    \n",
    "    # Pair each question with its corresponding answer and append to the QA pairs list\n",
    "    for i in range(len(question_list)):\n",
    "        qa_pairs.append({\"Question\": question_list[i], \"Answer\": answer_list[i]})\n",
    "    \n",
    "    # Return the list of question-answer pairs\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "def generate_all_qa_pairs():\n",
    "    \"\"\"\n",
    "    Generates QA pairs for all sections of text stored in a JSON file and saves them to a new file.\n",
    "\n",
    "    This function reads sections of text from a JSON file, generates QA pairs for each section\n",
    "    using the `generate_section_QA_pairs` function, and writes the resulting QA pairs to another\n",
    "    JSON file. The function also includes rate limiting to comply with API usage limits.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"     \n",
    "    \n",
    "    # Open and load the text data from the specified JSON file\n",
    "    with open(\"../data/all_pdf_text.json\", \"r\", encoding=\"utf-8\") as fin:\n",
    "        all_sections = json.load(fin)\n",
    "    \n",
    "    # Initialize an empty list to store all QA pairs\n",
    "    all_pairs = []\n",
    "\n",
    "    # Iterate over each section of text to generate QA pairs\n",
    "    for i in trange(len(all_sections)):\n",
    "        # Generate QA pairs for the current section\n",
    "        section_qa_pair = generate_section_QA_pairs(all_sections[i], client, GENERATE_QUESTION_PROMPT, GENERATE_ANSWER_PROMPT)\n",
    "        \n",
    "        # Extend the list of all QA pairs with the newly generated pairs\n",
    "        all_pairs.extend(section_qa_pair)\n",
    "        \n",
    "        # Write the QA pairs to a JSON file after each iteration to avoid data loss\n",
    "        with open(\"../data/QA_pairs.json\", \"w\", encoding='utf-8') as fout:\n",
    "            json.dump(all_pairs, fout, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        # Implement a delay after every 30 sections to avoid exceeding the API rate limit\n",
    "        if i > 29 and i % 30 == 0:\n",
    "            time.sleep(62)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_all_qa_pairs()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
